import os

import numpy as np

from glob import glob

from PIL import Image

from tqdm import tqdm



import torch

import torch.nn as nn

import torch.optim as optim

import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader

import torchvision.transforms as T



from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

import wandb



# -----------------------------

# Config

# -----------------------------

CONFIG = {

Â  Â  "image_size": 512,

Â  Â  "batch_size": 8,

Â  Â  "val_batch_size": 1,

Â  Â  "test_batch_size": 1,

Â  Â  "epochs": 100,

Â  Â  "lr": 1e-4,

Â  Â  "num_classes": 3,

Â  Â  "device": torch.device("cuda:0" if torch.cuda.is_available() else "cpu"),

Â  Â  "save_model_every": 10,Â  # Save model every N epochs

Â  Â  "test_every": 5,Â  Â  Â  Â  Â # Run test evaluation every N epochs

}



wandb.init(

Â  Â  project="segmentasi-jamur",

Â  Â  name="unet-fungi-v5-with-test",

Â  Â  config=CONFIG

)



# -----------------------------

# Dataset

# -----------------------------

class FungiDataset(Dataset):

Â  Â  def __init__(self, image_dir, mask_dir, transform=None, split_name="unknown"):

Â  Â  Â  Â  self.mask_paths = glob(os.path.join(mask_dir, "*.png"))

Â  Â  Â  Â  self.image_paths = []

Â  Â  Â  Â  self.transform = transform

Â  Â  Â  Â  self.split_name = split_name



Â  Â  Â  Â  valid_pairs = []

Â  Â  Â  Â  for mask_path in self.mask_paths:

Â  Â  Â  Â  Â  Â  base_name = os.path.splitext(os.path.basename(mask_path))[0]

Â  Â  Â  Â  Â  Â  image_path = os.path.join(image_dir, f"{base_name}.jpg")

Â  Â  Â  Â  Â  Â  if os.path.exists(image_path):

Â  Â  Â  Â  Â  Â  Â  Â  valid_pairs.append((image_path, mask_path))

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  print(f"[WARNING] Image not found for mask: {mask_path}")



Â  Â  Â  Â  if len(valid_pairs) == 0:

Â  Â  Â  Â  Â  Â  raise ValueError(f"No valid image-mask pairs found in {image_dir} and {mask_dir}")



Â  Â  Â  Â  self.image_paths, self.mask_paths = zip(*valid_pairs)

Â  Â  Â  Â  print(f"âœ… Loaded {len(self.image_paths)} image-mask pairs for {split_name} split")



Â  Â  def __len__(self):

Â  Â  Â  Â  return len(self.image_paths)



Â  Â  def __getitem__(self, idx):

Â  Â  Â  Â  image = Image.open(self.image_paths[idx]).convert("RGB")

Â  Â  Â  Â  mask = Image.open(self.mask_paths[idx]).convert("L")



Â  Â  Â  Â  if self.transform:

Â  Â  Â  Â  Â  Â  image = self.transform(image)

Â  Â  Â  Â  Â  Â  mask = T.Resize((CONFIG["image_size"], CONFIG["image_size"]))(mask)

Â  Â  Â  Â  Â  Â  mask = torch.from_numpy(np.array(mask, dtype=np.int64))



Â  Â  Â  Â  return image, mask



# -----------------------------

# U-Net with Interpolation

# -----------------------------

class DoubleConv(nn.Module):

Â  Â  def __init__(self, in_ch, out_ch):

Â  Â  Â  Â  super().__init__()

Â  Â  Â  Â  self.conv = nn.Sequential(

Â  Â  Â  Â  Â  Â  nn.Conv2d(in_ch, out_ch, 3, padding=1),

Â  Â  Â  Â  Â  Â  nn.ReLU(inplace=True),

Â  Â  Â  Â  Â  Â  nn.Conv2d(out_ch, out_ch, 3, padding=1),

Â  Â  Â  Â  Â  Â  nn.ReLU(inplace=True)

Â  Â  Â  Â  )



Â  Â  def forward(self, x):

Â  Â  Â  Â  return self.conv(x)



class UNet(nn.Module):

Â  Â  def __init__(self, n_classes):

Â  Â  Â  Â  super().__init__()

Â  Â  Â  Â  self.dconv_down1 = DoubleConv(3, 64)

Â  Â  Â  Â  self.dconv_down2 = DoubleConv(64, 128)

Â  Â  Â  Â  self.dconv_down3 = DoubleConv(128, 256)

Â  Â  Â  Â  self.dconv_down4 = DoubleConv(256, 512)



Â  Â  Â  Â  self.maxpool = nn.MaxPool2d(2)



Â  Â  Â  Â  self.dconv_up3 = DoubleConv(512 + 256, 256)

Â  Â  Â  Â  self.dconv_up2 = DoubleConv(256 + 128, 128)

Â  Â  Â  Â  self.dconv_up1 = DoubleConv(128 + 64, 64)



Â  Â  Â  Â  self.conv_last = nn.Conv2d(64, n_classes, 1)



Â  Â  def forward(self, x):

Â  Â  Â  Â  conv1 = self.dconv_down1(x)

Â  Â  Â  Â  x = self.maxpool(conv1)



Â  Â  Â  Â  conv2 = self.dconv_down2(x)

Â  Â  Â  Â  x = self.maxpool(conv2)



Â  Â  Â  Â  conv3 = self.dconv_down3(x)

Â  Â  Â  Â  x = self.maxpool(conv3)



Â  Â  Â  Â  x = self.dconv_down4(x)



Â  Â  Â  Â  x = F.interpolate(x, size=conv3.shape[2:], mode='bilinear', align_corners=False)

Â  Â  Â  Â  x = torch.cat([x, conv3], dim=1)



Â  Â  Â  Â  x = self.dconv_up3(x)

Â  Â  Â  Â  x = F.interpolate(x, size=conv2.shape[2:], mode='bilinear', align_corners=False)

Â  Â  Â  Â  x = torch.cat([x, conv2], dim=1)



Â  Â  Â  Â  x = self.dconv_up2(x)

Â  Â  Â  Â  x = F.interpolate(x, size=conv1.shape[2:], mode='bilinear', align_corners=False)

Â  Â  Â  Â  x = torch.cat([x, conv1], dim=1)



Â  Â  Â  Â  x = self.dconv_up1(x)

Â  Â  Â  Â  return self.conv_last(x)



# -----------------------------

# Metrics

# -----------------------------

def calculate_metrics(pred, target):

Â  Â  pred = pred.view(-1).cpu().numpy()

Â  Â  target = target.view(-1).cpu().numpy()



Â  Â  precision = precision_score(target, pred, average='macro', zero_division=0)

Â  Â  recall = recall_score(target, pred, average='macro', zero_division=0)

Â  Â  f1 = f1_score(target, pred, average='macro', zero_division=0)

Â  Â  acc = accuracy_score(target, pred)

Â  Â  return precision, recall, f1, acc



def calculate_per_class_metrics(pred, target, num_classes):

Â  Â  """Calculate per-class metrics for detailed analysis"""

Â  Â  pred = pred.view(-1).cpu().numpy()

Â  Â  target = target.view(-1).cpu().numpy()

Â  Â Â 

Â  Â  per_class_metrics = {}

Â  Â  for class_id in range(num_classes):

Â  Â  Â  Â  # Create binary masks for current class

Â  Â  Â  Â  pred_binary = (pred == class_id).astype(int)

Â  Â  Â  Â  target_binary = (target == class_id).astype(int)

Â  Â  Â  Â Â 

Â  Â  Â  Â  if target_binary.sum() > 0:Â  # Only calculate if class exists in target

Â  Â  Â  Â  Â  Â  precision = precision_score(target_binary, pred_binary, zero_division=0)

Â  Â  Â  Â  Â  Â  recall = recall_score(target_binary, pred_binary, zero_division=0)

Â  Â  Â  Â  Â  Â  f1 = f1_score(target_binary, pred_binary, zero_division=0)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  per_class_metrics[f'class_{class_id}'] = {

Â  Â  Â  Â  Â  Â  Â  Â  'precision': precision,

Â  Â  Â  Â  Â  Â  Â  Â  'recall': recall,

Â  Â  Â  Â  Â  Â  Â  Â  'f1': f1

Â  Â  Â  Â  Â  Â  }

Â  Â Â 

Â  Â  return per_class_metrics



# -----------------------------

# Train, Validate & Test Functions

# -----------------------------

def train_one_epoch(model, loader, optimizer, loss_fn, device):

Â  Â  model.train()

Â  Â  total_loss = 0.0

Â  Â  all_preds, all_targets = [], []



Â  Â  for i, (images, masks) in enumerate(tqdm(loader, desc="Training", leave=False)):

Â  Â  Â  Â  images, masks = images.to(device), masks.to(device)



Â  Â  Â  Â  optimizer.zero_grad()

Â  Â  Â  Â  outputs = model(images)



Â  Â  Â  Â  if torch.isnan(outputs).any():

Â  Â  Â  Â  Â  Â  print(f"[NaN DETECTED] in outputs at batch {i}")

Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  loss = loss_fn(outputs, masks)

Â  Â  Â  Â  loss.backward()

Â  Â  Â  Â  optimizer.step()



Â  Â  Â  Â  total_loss += loss.item()

Â  Â  Â  Â  preds = torch.argmax(outputs, dim=1)

Â  Â  Â  Â  all_preds.append(preds.cpu())

Â  Â  Â  Â  all_targets.append(masks.cpu())



Â  Â  all_preds = torch.cat(all_preds)

Â  Â  all_targets = torch.cat(all_targets)

Â  Â  prec, rec, f1, acc = calculate_metrics(all_preds, all_targets)



Â  Â  return total_loss / len(loader), prec, rec, f1, acc



def validate(model, loader, loss_fn, device, split_name="validation"):

Â  Â  model.eval()

Â  Â  total_loss = 0.0

Â  Â  all_preds, all_targets = [], []



Â  Â  with torch.no_grad():

Â  Â  Â  Â  for i, (images, masks) in enumerate(tqdm(loader, desc=f"{split_name.capitalize()}", leave=False)):

Â  Â  Â  Â  Â  Â  images, masks = images.to(device), masks.to(device)



Â  Â  Â  Â  Â  Â  outputs = model(images)



Â  Â  Â  Â  Â  Â  if torch.isnan(outputs).any():

Â  Â  Â  Â  Â  Â  Â  Â  print(f"[NaN DETECTED] in {split_name} batch {i}")

Â  Â  Â  Â  Â  Â  Â  Â  continue



Â  Â  Â  Â  Â  Â  loss = loss_fn(outputs, masks)

Â  Â  Â  Â  Â  Â  total_loss += loss.item()



Â  Â  Â  Â  Â  Â  preds = torch.argmax(outputs, dim=1)

Â  Â  Â  Â  Â  Â  all_preds.append(preds.cpu())

Â  Â  Â  Â  Â  Â  all_targets.append(masks.cpu())



Â  Â  Â  Â  Â  Â  if i % 10 == 0 and split_name == "validation":

Â  Â  Â  Â  Â  Â  Â  Â  print(f"[{split_name.capitalize()} Batch {i}] loss={loss.item():.4f}, shape={outputs.shape}")



Â  Â  all_preds = torch.cat(all_preds)

Â  Â  all_targets = torch.cat(all_targets)

Â  Â  prec, rec, f1, acc = calculate_metrics(all_preds, all_targets)

Â  Â Â 

Â  Â  # Calculate per-class metrics for detailed analysis

Â  Â  per_class_metrics = calculate_per_class_metrics(all_preds, all_targets, CONFIG["num_classes"])



Â  Â  return total_loss / len(loader), prec, rec, f1, acc, per_class_metrics



def test_model(model, test_loader, loss_fn, device):

Â  Â  """Dedicated test function with detailed logging"""

Â  Â  print("\nğŸ§ª Running Test Evaluation...")

Â  Â  test_loss, test_prec, test_rec, test_f1, test_acc, per_class_metrics = validate(

Â  Â  Â  Â  model, test_loader, loss_fn, device, "test"

Â  Â  )

Â  Â Â 

Â  Â  print(f"ğŸ“Š Test Results:")

Â  Â  print(f"Â  Â Loss: {test_loss:.4f}")

Â  Â  print(f"Â  Â Accuracy: {test_acc:.4f}")

Â  Â  print(f"Â  Â Precision: {test_prec:.4f}")

Â  Â  print(f"Â  Â Recall: {test_rec:.4f}")

Â  Â  print(f"Â  Â F1-Score: {test_f1:.4f}")

Â  Â Â 

Â  Â  # Log per-class metrics

Â  Â  print(f"ğŸ“‹ Per-Class Metrics:")

Â  Â  for class_name, metrics in per_class_metrics.items():

Â  Â  Â  Â  print(f"Â  Â {class_name}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")

Â  Â Â 

Â  Â  return test_loss, test_prec, test_rec, test_f1, test_acc, per_class_metrics



# -----------------------------

# Model Saving & Loading

# -----------------------------

def save_model_checkpoint(model, optimizer, epoch, best_val_f1, filepath):

Â  Â  """Save model checkpoint with training state"""

Â  Â  torch.save({

Â  Â  Â  Â  'epoch': epoch,

Â  Â  Â  Â  'model_state_dict': model.state_dict(),

Â  Â  Â  Â  'optimizer_state_dict': optimizer.state_dict(),

Â  Â  Â  Â  'best_val_f1': best_val_f1,

Â  Â  Â  Â  'config': CONFIG

Â  Â  }, filepath)

Â  Â  print(f"ğŸ’¾ Checkpoint saved to {filepath}")



def save_best_model(model, filepath):

Â  Â  """Save only the model weights (for inference)"""

Â  Â  torch.save(model.state_dict(), filepath)

Â  Â  print(f"ğŸ† Best model saved to {filepath}")



# -----------------------------

# Main Training Loop

# -----------------------------

def main():

Â  Â  transform = T.Compose([

Â  Â  Â  Â  T.Resize((CONFIG["image_size"], CONFIG["image_size"])),

Â  Â  Â  Â  T.ToTensor(),

Â  Â  ])



Â  Â  # Load all datasets

Â  Â  print("ğŸ“‚ Loading datasets...")

Â  Â  train_dataset = FungiDataset("dataset_masks/train/images", "dataset_masks/train/label_masks", transform, "train")

Â  Â  val_dataset = FungiDataset("dataset_masks/valid/images", "dataset_masks/valid/label_masks", transform, "validation")

Â  Â  test_dataset = FungiDataset("dataset_masks/test/images", "dataset_masks/test/label_masks", transform, "test")



Â  Â  train_loader = DataLoader(train_dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=4)

Â  Â  val_loader = DataLoader(val_dataset, batch_size=CONFIG["val_batch_size"], shuffle=False, num_workers=2)

Â  Â  test_loader = DataLoader(test_dataset, batch_size=CONFIG["test_batch_size"], shuffle=False, num_workers=2)



Â  Â  # Initialize model and training components

Â  Â  model = UNet(n_classes=CONFIG["num_classes"]).to(CONFIG["device"])

Â  Â  optimizer = optim.Adam(model.parameters(), lr=CONFIG["lr"])

Â  Â  criterion = nn.CrossEntropyLoss()

Â  Â Â 

Â  Â  # Training tracking variables

Â  Â  best_val_f1 = 0.0

Â  Â  best_test_f1 = 0.0

Â  Â Â 

Â  Â  print(f"ğŸš€ Starting training for {CONFIG['epochs']} epochs...")

Â  Â  print(f"ğŸ“Š Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")



Â  Â  for epoch in range(CONFIG["epochs"]):

Â  Â  Â  Â  print(f"\nğŸ“… Epoch {epoch+1}/{CONFIG['epochs']}")



Â  Â  Â  Â  # Training

Â  Â  Â  Â  train_loss, train_prec, train_rec, train_f1, train_acc = train_one_epoch(

Â  Â  Â  Â  Â  Â  model, train_loader, optimizer, criterion, CONFIG["device"]

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Validation

Â  Â  Â  Â  val_loss, val_prec, val_rec, val_f1, val_acc, val_per_class = validate(

Â  Â  Â  Â  Â  Â  model, val_loader, criterion, CONFIG["device"], "validation"

Â  Â  Â  Â  )



Â  Â  Â  Â  # Initialize wandb log with training and validation metrics

Â  Â  Â  Â  wandb_log = {

Â  Â  Â  Â  Â  Â  "epoch": epoch + 1,

Â  Â  Â  Â  Â  Â  "train/loss": train_loss,

Â  Â  Â  Â  Â  Â  "train/precision": train_prec,

Â  Â  Â  Â  Â  Â  "train/recall": train_rec,

Â  Â  Â  Â  Â  Â  "train/f1": train_f1,

Â  Â  Â  Â  Â  Â  "train/accuracy": train_acc,

Â  Â  Â  Â  Â  Â  "val/loss": val_loss,

Â  Â  Â  Â  Â  Â  "val/precision": val_prec,

Â  Â  Â  Â  Â  Â  "val/recall": val_rec,

Â  Â  Â  Â  Â  Â  "val/f1": val_f1,

Â  Â  Â  Â  Â  Â  "val/accuracy": val_acc,

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Add per-class validation metrics to wandb

Â  Â  Â  Â  for class_name, metrics in val_per_class.items():

Â  Â  Â  Â  Â  Â  wandb_log[f"val_per_class/{class_name}_precision"] = metrics['precision']

Â  Â  Â  Â  Â  Â  wandb_log[f"val_per_class/{class_name}_recall"] = metrics['recall']

Â  Â  Â  Â  Â  Â  wandb_log[f"val_per_class/{class_name}_f1"] = metrics['f1']



Â  Â  Â  Â  # Test evaluation (every N epochs or last epoch)

Â  Â  Â  Â  if (epoch + 1) % CONFIG["test_every"] == 0 or epoch == CONFIG["epochs"] - 1:

Â  Â  Â  Â  Â  Â  test_loss, test_prec, test_rec, test_f1, test_acc, test_per_class = test_model(

Â  Â  Â  Â  Â  Â  Â  Â  model, test_loader, criterion, CONFIG["device"]

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Add test metrics to main wandb log

Â  Â  Â  Â  Â  Â  wandb_log.update({

Â  Â  Â  Â  Â  Â  Â  Â  "test/loss": test_loss,

Â  Â  Â  Â  Â  Â  Â  Â  "test/precision": test_prec,

Â  Â  Â  Â  Â  Â  Â  Â  "test/recall": test_rec,

Â  Â  Â  Â  Â  Â  Â  Â  "test/f1": test_f1,

Â  Â  Â  Â  Â  Â  Â  Â  "test/accuracy": test_acc,

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Add per-class test metrics

Â  Â  Â  Â  Â  Â  for class_name, metrics in test_per_class.items():

Â  Â  Â  Â  Â  Â  Â  Â  wandb_log[f"test_per_class/{class_name}_precision"] = metrics['precision']

Â  Â  Â  Â  Â  Â  Â  Â  wandb_log[f"test_per_class/{class_name}_recall"] = metrics['recall']

Â  Â  Â  Â  Â  Â  Â  Â  wandb_log[f"test_per_class/{class_name}_f1"] = metrics['f1']

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Update best test score

Â  Â  Â  Â  Â  Â  if test_f1 > best_test_f1:

Â  Â  Â  Â  Â  Â  Â  Â  best_test_f1 = test_f1

Â  Â  Â  Â  Â  Â  Â  Â  wandb_log["best_test_f1"] = best_test_f1

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  # If no test evaluation this epoch, still log the current best test F1

Â  Â  Â  Â  Â  Â  wandb_log["best_test_f1"] = best_test_f1



Â  Â  Â  Â  # Log to wandb

Â  Â  Â  Â  wandb.log(wandb_log)



Â  Â  Â  Â  # Save best model based on validation F1

Â  Â  Â  Â  if val_f1 > best_val_f1:

Â  Â  Â  Â  Â  Â  best_val_f1 = val_f1

Â  Â  Â  Â  Â  Â  save_best_model(model, "unet_fungi_best_model.pth")

Â  Â  Â  Â  Â  Â  wandb.log({"best_val_f1": best_val_f1})



Â  Â  Â  Â  # Save checkpoint periodically

Â  Â  Â  Â  if (epoch + 1) % CONFIG["save_model_every"] == 0:

Â  Â  Â  Â  Â  Â  checkpoint_path = f"unet_fungi_checkpoint_epoch_{epoch+1}.pth"

Â  Â  Â  Â  Â  Â  save_model_checkpoint(model, optimizer, epoch, best_val_f1, checkpoint_path)



Â  Â  Â  Â  print(f"ğŸ“ˆ Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Best Val F1: {best_val_f1:.4f}")



Â  Â  # Final test evaluation

Â  Â  print("\nğŸ¯ Final Test Evaluation...")

Â  Â  final_test_loss, final_test_prec, final_test_rec, final_test_f1, final_test_acc, final_test_per_class = test_model(

Â  Â  Â  Â  model, test_loader, criterion, CONFIG["device"]

Â  Â  )

Â  Â Â 

Â  Â  # Log final test results

Â  Â  final_log = {

Â  Â  Â  Â  "final_test/loss": final_test_loss,

Â  Â  Â  Â  "final_test/precision": final_test_prec,

Â  Â  Â  Â  "final_test/recall": final_test_rec,

Â  Â  Â  Â  "final_test/f1": final_test_f1,

Â  Â  Â  Â  "final_test/accuracy": final_test_acc,

Â  Â  }

Â  Â Â 

Â  Â  for class_name, metrics in final_test_per_class.items():

Â  Â  Â  Â  final_log[f"final_test_per_class/{class_name}_precision"] = metrics['precision']

Â  Â  Â  Â  final_log[f"final_test_per_class/{class_name}_recall"] = metrics['recall']

Â  Â  Â  Â  final_log[f"final_test_per_class/{class_name}_f1"] = metrics['f1']

Â  Â Â 

Â  Â  wandb.log(final_log)



Â  Â  # Save final model

Â  Â  torch.save(model.state_dict(), "unet_fungi_segmentation_final.pth")

Â  Â  print("âœ… Final model saved to unet_fungi_segmentation_final.pth")

Â  Â Â 

Â  Â  # Create summary table for wandb

Â  Â  summary_table = wandb.Table(columns=["Metric", "Train", "Validation", "Test"])

Â  Â  summary_table.add_data("Loss", f"{train_loss:.4f}", f"{val_loss:.4f}", f"{final_test_loss:.4f}")

Â  Â  summary_table.add_data("Accuracy", f"{train_acc:.4f}", f"{val_acc:.4f}", f"{final_test_acc:.4f}")

Â  Â  summary_table.add_data("Precision", f"{train_prec:.4f}", f"{val_prec:.4f}", f"{final_test_prec:.4f}")

Â  Â  summary_table.add_data("Recall", f"{train_rec:.4f}", f"{val_rec:.4f}", f"{final_test_rec:.4f}")

Â  Â  summary_table.add_data("F1-Score", f"{train_f1:.4f}", f"{val_f1:.4f}", f"{final_test_f1:.4f}")

Â  Â Â 

Â  Â  wandb.log({"training_summary": summary_table})

Â  Â Â 

Â  Â  print(f"\nğŸ Training completed!")

Â  Â  print(f"ğŸ“Š Final Results:")

Â  Â  print(f"Â  Â Best Validation F1: {best_val_f1:.4f}")

Â  Â  print(f"Â  Â Best Test F1: {best_test_f1:.4f}")

Â  Â  print(f"Â  Â Final Test F1: {final_test_f1:.4f}")



if __name__ == "__main__":

Â  Â  main()
